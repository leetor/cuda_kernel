// leetgpu percentile 99.3%

#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <iostream>
#include <mma.h>

using namespace nvcuda;

// Tile sizes for the GEMM kernel
#define BM 128   // Block tile M
#define BN 256   // Block tile N
#define BK 32    // Block tile K

// Padding to avoid shared memory bank conflicts
#define APAD 8
#define BPAD 8

#ifndef OFFSET
#define OFFSET(row, col, ld) ((row) * (ld) + (col))
#endif

#ifndef FLOAT4
#define FLOAT4(pointer) (reinterpret_cast<float4*>(&(pointer))[0])
#endif

#ifndef FLOAT4_READ
#define FLOAT4_READ(pointer) (reinterpret_cast<const float4*>(&(pointer))[0])
#endif
__global__ void myHGEMMAlignedV1(
    const half * __restrict__ a, const half * __restrict__ b, half * __restrict__ c,
    const int M, const int N, const int K, float alpha, float beta) {

    // Block indices
    int bx = blockIdx.x;  // Block column
    int by = blockIdx.y;  // Block row
    
    // Thread indices (1D block with 256 threads = 8 warps)
    int tid = threadIdx.x;
    int wid = tid >> 5;   // Warp ID (0-7)
    int lane = tid & 31;  // Lane ID within warp (0-31)

    // Shared memory for tiles
    __shared__ half s_a[BM][BK + APAD];  // 128 x 40
    __shared__ half s_b[BK][BN + BPAD];  // 32 x 264

    // WMMA fragments
    // Each warp computes a 64x64 tile, using 4x4 = 16 WMMA 16x16 tiles
    // Use FLOAT accumulator for better precision during computation
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> frag_a[2][4];
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> frag_b[2][4];
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> frag_c[4][4];  // float accumulator

    // Initialize accumulators to zero
    #pragma unroll
    for (int i = 0; i < 4; i++) {
        #pragma unroll
        for (int j = 0; j < 4; j++) {
            wmma::fill_fragment(frag_c[i][j], 0.0f);
        }
    }

    // Calculate loading positions for A and B
    // Each thread loads 8 half values (float4) at a time
    // 256 threads load 128x32 = 4096 halves for A, each loads 16 halves (2 rows x 8 cols)
    // 256 threads load 32x256 = 8192 halves for B, each loads 32 halves (4 rows x 8 cols)
    
    int load_a_smem_m = (tid >> 2) << 1;  // Which row pair (0,2,4,...,126)
    int load_a_smem_k = (tid & 3) << 3;   // Which 8-column group (0,8,16,24)
    
    int load_b_smem_k = (tid >> 5) << 2;  // Which 4-row group (0,4,8,...,28)
    int load_b_smem_n = (tid & 31) << 3;  // Which 8-column group (0,8,16,...,248)

    // Global memory positions
    int load_a_gmem_m = by * BM + load_a_smem_m;
    int load_b_gmem_n = bx * BN + load_b_smem_n;

    // Which warp computes which 64x64 output tile
    // 8 warps: 2 rows x 4 cols of 64x64 tiles
    int comp_c_frag_m = wid & 1;   // 0 or 1
    int comp_c_frag_n = wid >> 1;  // 0, 1, 2, or 3

    // ================= Main Loop over K dimension =================
    // Use ceiling division to handle K not divisible by BK
    int num_k_tiles = (K + BK - 1) / BK;
    
    for (int bk = 0; bk < num_k_tiles; bk++) {
        int k_offset = bk * BK;
        
        // Current global K position for this thread's load
        int load_a_gmem_k = k_offset + load_a_smem_k;
        int load_b_gmem_k = k_offset + load_b_smem_k;
        
        // ===== Load A tile (128 x 32) =====
        // Each thread loads 2 rows x 8 columns = 16 halves
        int a_row0 = load_a_gmem_m;
        int a_row1 = load_a_gmem_m + 1;
        
        // Check bounds for M and K dimensions
        bool valid_a_row0 = (a_row0 < M) && (load_a_gmem_k + 7 < K);
        bool valid_a_row1 = (a_row1 < M) && (load_a_gmem_k + 7 < K);
        
        if (valid_a_row0) {
            FLOAT4(s_a[load_a_smem_m][load_a_smem_k]) = 
                FLOAT4_READ(a[OFFSET(a_row0, load_a_gmem_k, K)]);
        } else if (a_row0 < M && load_a_gmem_k < K) {
            // Partial load - element by element
            #pragma unroll
            for (int i = 0; i < 8; i++) {
                if (load_a_gmem_k + i < K) {
                    s_a[load_a_smem_m][load_a_smem_k + i] = a[OFFSET(a_row0, load_a_gmem_k + i, K)];
                } else {
                    s_a[load_a_smem_m][load_a_smem_k + i] = __float2half(0.0f);
                }
            }
        } else {
            FLOAT4(s_a[load_a_smem_m][load_a_smem_k]) = make_float4(0, 0, 0, 0);
        }

        if (valid_a_row1) {
            FLOAT4(s_a[load_a_smem_m + 1][load_a_smem_k]) = 
                FLOAT4_READ(a[OFFSET(a_row1, load_a_gmem_k, K)]);
        } else if (a_row1 < M && load_a_gmem_k < K) {
            #pragma unroll
            for (int i = 0; i < 8; i++) {
                if (load_a_gmem_k + i < K) {
                    s_a[load_a_smem_m + 1][load_a_smem_k + i] = a[OFFSET(a_row1, load_a_gmem_k + i, K)];
                } else {
                    s_a[load_a_smem_m + 1][load_a_smem_k + i] = __float2half(0.0f);
                }
            }
        } else {
            FLOAT4(s_a[load_a_smem_m + 1][load_a_smem_k]) = make_float4(0, 0, 0, 0);
        }

        // ===== Load B tile (32 x 256) =====
        // Each thread loads 4 rows x 8 columns = 32 halves
        bool valid_b_n = (load_b_gmem_n + 7 < N);
        
        #pragma unroll
        for (int row = 0; row < 4; row++) {
            int b_k = load_b_gmem_k + row;
            int smem_k = load_b_smem_k + row;
            
            if (b_k < K && valid_b_n) {
                FLOAT4(s_b[smem_k][load_b_smem_n]) = 
                    FLOAT4_READ(b[OFFSET(b_k, load_b_gmem_n, N)]);
            } else if (b_k < K && load_b_gmem_n < N) {
                // Partial load
                #pragma unroll
                for (int i = 0; i < 8; i++) {
                    if (load_b_gmem_n + i < N) {
                        s_b[smem_k][load_b_smem_n + i] = b[OFFSET(b_k, load_b_gmem_n + i, N)];
                    } else {
                        s_b[smem_k][load_b_smem_n + i] = __float2half(0.0f);
                    }
                }
            } else {
                FLOAT4(s_b[smem_k][load_b_smem_n]) = make_float4(0, 0, 0, 0);
            }
        }

        __syncthreads();

        // ===== WMMA Computation =====
        // Load A fragments (4 x 16x16 tiles along M, 2 x 16x16 tiles along K)
        wmma::load_matrix_sync(frag_a[0][0], &s_a[comp_c_frag_m * 64     ][ 0], BK + APAD);
        wmma::load_matrix_sync(frag_a[0][1], &s_a[comp_c_frag_m * 64 + 16][ 0], BK + APAD);
        wmma::load_matrix_sync(frag_a[0][2], &s_a[comp_c_frag_m * 64 + 32][ 0], BK + APAD);
        wmma::load_matrix_sync(frag_a[0][3], &s_a[comp_c_frag_m * 64 + 48][ 0], BK + APAD);
        wmma::load_matrix_sync(frag_a[1][0], &s_a[comp_c_frag_m * 64     ][16], BK + APAD);
        wmma::load_matrix_sync(frag_a[1][1], &s_a[comp_c_frag_m * 64 + 16][16], BK + APAD);
        wmma::load_matrix_sync(frag_a[1][2], &s_a[comp_c_frag_m * 64 + 32][16], BK + APAD);
        wmma::load_matrix_sync(frag_a[1][3], &s_a[comp_c_frag_m * 64 + 48][16], BK + APAD);

        // Load B fragments (2 x 16x16 tiles along K, 4 x 16x16 tiles along N)
        wmma::load_matrix_sync(frag_b[0][0], &s_b[ 0][comp_c_frag_n * 64     ], BN + BPAD);
        wmma::load_matrix_sync(frag_b[0][1], &s_b[ 0][comp_c_frag_n * 64 + 16], BN + BPAD);
        wmma::load_matrix_sync(frag_b[0][2], &s_b[ 0][comp_c_frag_n * 64 + 32], BN + BPAD);
        wmma::load_matrix_sync(frag_b[0][3], &s_b[ 0][comp_c_frag_n * 64 + 48], BN + BPAD);
        wmma::load_matrix_sync(frag_b[1][0], &s_b[16][comp_c_frag_n * 64     ], BN + BPAD);
        wmma::load_matrix_sync(frag_b[1][1], &s_b[16][comp_c_frag_n * 64 + 16], BN + BPAD);
        wmma::load_matrix_sync(frag_b[1][2], &s_b[16][comp_c_frag_n * 64 + 32], BN + BPAD);
        wmma::load_matrix_sync(frag_b[1][3], &s_b[16][comp_c_frag_n * 64 + 48], BN + BPAD);

        // Matrix multiply-accumulate
        #pragma unroll
        for (int i = 0; i < 4; i++) {
            #pragma unroll
            for (int j = 0; j < 4; j++) {
                wmma::mma_sync(frag_c[i][j], frag_a[0][i], frag_b[0][j], frag_c[i][j]);
                wmma::mma_sync(frag_c[i][j], frag_a[1][i], frag_b[1][j], frag_c[i][j]);
            }
        }

        __syncthreads();
    }

    // ================= Epilogue: Store C =================
    // Ensure all computation is complete before reusing shared memory
    __syncthreads(); 

    int store_c_gmem_m = by * BM + comp_c_frag_m * 64;
    int store_c_gmem_n = bx * BN + comp_c_frag_n * 64;

    // Keep alpha and beta as float for precision!
    // frag_c is already float accumulator, so we use float arithmetic

    // Reuse s_a as staging area for boundary handling
    // For float storage, we need 256 floats = 1024 bytes per warp
    // s_a has 128*40*2 = 10240 bytes, 8 warps need 8192 bytes, fits fine
    float* smem_stage_f = (float*)s_a;
    float* warp_smem_f = smem_stage_f + wid * 256;
    
    // Also need half staging area for final conversion
    half* smem_stage_h = (half*)s_a;
    half* warp_smem_h = smem_stage_h + wid * 256;

    #pragma unroll
    for (int i = 0; i < 4; i++) {
        #pragma unroll
        for (int j = 0; j < 4; j++) {
            int tile_row = store_c_gmem_m + i * 16;
            int tile_col = store_c_gmem_n + j * 16;
            
            // Check if entire 16x16 tile is within matrix bounds
            bool is_safe_tile = (tile_row + 16 <= M) && (tile_col + 16 <= N);

            // Apply alpha scaling in float precision
            #pragma unroll
            for (int t = 0; t < frag_c[i][j].num_elements; t++) {
                frag_c[i][j].x[t] *= alpha;
            }

            if (is_safe_tile) {
                // === Fast path: Tile fully within bounds ===
                half* dst_ptr = &c[OFFSET(tile_row, tile_col, N)];

                if (beta != 0.0f) {
                    // Load old C as half accumulator first (WMMA requirement)
                    wmma::fragment<wmma::accumulator, 16, 16, 16, half> frag_old_h;
                    wmma::load_matrix_sync(frag_old_h, dst_ptr, N, wmma::mem_row_major);
                    // Convert to float and apply beta in float precision
                    #pragma unroll
                    for (int t = 0; t < frag_old_h.num_elements; t++) {
                        frag_c[i][j].x[t] += __half2float(frag_old_h.x[t]) * beta;
                    }
                }
                
                // Store float accumulator to shared memory first
                wmma::store_matrix_sync(warp_smem_f, frag_c[i][j], 16, wmma::mem_row_major);
                __syncwarp();
                
                // Convert float to half and write to global memory
                #pragma unroll
                for (int k = 0; k < 8; ++k) {
                    int idx = lane + k * 32;
                    int r = idx / 16;
                    int col = idx % 16;
                    c[OFFSET(tile_row + r, tile_col + col, N)] = __float2half(warp_smem_f[idx]);
                }
            } 
            else {
                // === Slow path: Boundary handling via shared memory ===
                
                // Handle beta: load old C values safely
                if (beta != 0.0f) {
                    // 1. Cooperatively load C_old from global to shared (as float)
                    // 32 threads in warp, each loads 8 elements = 256 total
                    #pragma unroll
                    for (int k = 0; k < 8; ++k) {
                        int idx = lane + k * 32;
                        int r = idx / 16;
                        int col = idx % 16;
                        int gr = tile_row + r;
                        int gc = tile_col + col;
                        
                        if (gr < M && gc < N) {
                            // Load half, convert to float for staging
                            warp_smem_f[idx] = __half2float(c[OFFSET(gr, gc, N)]);
                        } else {
                            warp_smem_f[idx] = 0.0f;
                        }
                    }
                    
                    // Ensure all threads in warp have finished writing to smem
                    __syncwarp();
                    
                    // Load from smem to float fragment
                    wmma::fragment<wmma::accumulator, 16, 16, 16, float> frag_old;
                    wmma::load_matrix_sync(frag_old, warp_smem_f, 16, wmma::mem_row_major);

                    // Apply beta in float precision
                    #pragma unroll
                    for (int t = 0; t < frag_old.num_elements; t++) {
                        frag_c[i][j].x[t] += frag_old.x[t] * beta;
                    }
                }

                // 2. Store float result to shared memory
                wmma::store_matrix_sync(warp_smem_f, frag_c[i][j], 16, wmma::mem_row_major);
                
                // Ensure wmma store is complete
                __syncwarp();

                // 3. Cooperatively write from shared to global with bounds check
                //    Convert float -> half during write
                #pragma unroll
                for (int k = 0; k < 8; ++k) {
                    int idx = lane + k * 32;
                    int r = idx / 16;
                    int col = idx % 16;
                    int gr = tile_row + r;
                    int gc = tile_col + col;
                    
                    if (gr < M && gc < N) {
                        c[OFFSET(gr, gc, N)] = __float2half(warp_smem_f[idx]);
                    }
                }
            }
        }
    }
}
/**
 * Host function to call the kernel
 * 
 * Computes: C = alpha * A * B + beta * C
 * Where: A is M x K, B is K x N, C is M x N
 */
extern "C" void solve(const half* A, const half* B, half* C, int M, int N, int K, 
                      float alpha, float beta) {
    
    // Block dimension: 256 threads (8 warps) in 1D
    // Each block computes a BM x BN = 128 x 256 output tile
    dim3 blockDim(256);

    // Grid dimension: cover the entire output matrix
    // gridDim.x = number of column tiles (along N)
    // gridDim.y = number of row tiles (along M)
    dim3 gridDim((N + BN - 1) / BN,   // Ceiling division for columns
                 (M + BM - 1) / BM);  // Ceiling division for rows

    // Launch kernel
    myHGEMMAlignedV1<<<gridDim, blockDim>>>(A, B, C, M, N, K, alpha, beta);

    // Check for launch errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cerr << "CUDA Kernel Launch Error: " << cudaGetErrorString(err) << std::endl;
    }
    
    // Synchronize to ensure computation is complete
    cudaDeviceSynchronize();
    
    // Check for execution errors
    err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cerr << "CUDA Kernel Execution Error: " << cudaGetErrorString(err) << std::endl;
    }
}